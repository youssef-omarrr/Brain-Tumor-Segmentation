{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c29688d4",
   "metadata": {},
   "source": [
    "# Brain tumor segmentation model using MONAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c30c05",
   "metadata": {},
   "source": [
    "## Quick overview of steps:\n",
    "\n",
    "1. Build file lists (images, masks).\n",
    "1. Build MONAI transforms for images & masks.\n",
    "1. Create Dataset / Dataloader (CacheDataset for speed).\n",
    "1. Create model (MONAI UNet for 2D).\n",
    "1. Load pretrained checkpoint (if available) into model.\n",
    "1. Define loss, optimizer, scheduler.\n",
    "1. Train (with validation, metrics, checkpointing).\n",
    "1. Fine-tuning tips (freeze, low LR, augmentations, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fc18a1",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37d58d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda'), '2.6.0+cu126')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "\n",
    "from monai.transforms import (\n",
    "    LoadImaged, EnsureChannelFirstd, ScaleIntensityd, ToTensord,\n",
    "    RandFlipd, RandRotate90d, Compose, Resized\n",
    ")\n",
    "from monai.data import PersistentDataset\n",
    "from monai.networks.nets import UNet\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "set_determinism(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Frees unused memory held by the caching allocator\n",
    "torch.cuda.empty_cache()  \n",
    "device, torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "898de07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 13 10:06:33 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 576.88                 Driver Version: 576.88         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   57C    P5             20W /  115W |     324MiB /   6144MiB |     20%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A           19820    C+G   ...\\app-1.20.1\\Flow.Launcher.exe      N/A      |\n",
      "|    0   N/A  N/A           21676    C+G   ...ms\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2585f80f",
   "metadata": {},
   "source": [
    "## 1. file lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bde988f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from training set: {'image': WindowsPath('brain_tumor_dataset/images/1041.png'), 'mask': WindowsPath('brain_tumor_dataset/masks/1041.png')}\n",
      "Sample from testing set: {'image': WindowsPath('brain_tumor_dataset/images/1284.png'), 'mask': WindowsPath('brain_tumor_dataset/masks/1284.png')}\n",
      "Length of training set: 2451\n",
      "Length of testing set: 613\n"
     ]
    }
   ],
   "source": [
    "# Define paths to the image and mask directories\n",
    "IMAGE_DIR = Path(\"brain_tumor_dataset/images\")\n",
    "MASK_DIR = Path(\"brain_tumor_dataset/masks\")\n",
    "\n",
    "# Get all image and mask file paths, assuming they are PNG files\n",
    "images = list(IMAGE_DIR.glob(\"*.png\"))\n",
    "masks = list(MASK_DIR.glob(\"*.png\"))\n",
    "\n",
    "# Create a list of dictionaries, pairing each image with its corresponding mask.\n",
    "# This assumes that the files in both directories are sorted in a corresponding order.\n",
    "data_dicts = [\n",
    "    {\n",
    "        \"image\": img, \n",
    "        \"mask\": m\n",
    "        \n",
    "    } for img, m in zip(images, masks)\n",
    "]\n",
    "\n",
    "# --- Data Splitting ---\n",
    "import random\n",
    "# Set a random seed for reproducibility, ensuring the shuffle is the same every time.\n",
    "random.seed(42) \n",
    "# Shuffle the data dictionaries in place to randomize the dataset.\n",
    "random.shuffle(data_dicts)\n",
    "\n",
    "# Calculate the split index for an 80/20 train-test split.\n",
    "n_val = int(0.8 * len(data_dicts)) \n",
    "# Split the data into training and testing sets.\n",
    "train_files = data_dicts[:n_val]\n",
    "test_files = data_dicts[n_val:]\n",
    "\n",
    "# Print a sample from each set to verify the structure.\n",
    "print(\"Sample from training set:\", train_files[5])\n",
    "print(\"Sample from testing set:\", test_files[5])\n",
    "print (f\"Length of training set: {len(train_files)}\")\n",
    "print (f\"Length of testing set: {len(test_files)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84c2d98",
   "metadata": {},
   "source": [
    "## 2. transforms 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8712815c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<monai.transforms.compose.Compose at 0x2b66edbbb20>,\n",
       " <monai.transforms.compose.Compose at 0x2b66e8567a0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target size\n",
    "target_size = (256, 256) # change if it doesn'tfit the GPU\n",
    "\n",
    "# Define the sequence of transformations for the training data. These include data augmentation steps.\n",
    "train_transforms = Compose([ \n",
    "    LoadImaged(keys=[\"image\", \"mask\"]),           # Loads the image and mask data from the file paths specified in the dictionary.\n",
    "    \n",
    "    EnsureChannelFirstd(keys=[\"image\", \"mask\"]),  # Ensures the data format is [Channel, Height, Width], which is standard for PyTorch.\n",
    "    \n",
    "    ScaleIntensityd(keys=[\"image\"]),              # Normalizes the intensity values of the image (e.g., to a [0, 1] range). \n",
    "    \n",
    "    Resized(keys=[\"image\", \"mask\"],               # Resize all image to the same shape\n",
    "            spatial_size=target_size),\n",
    "    \n",
    "    RandFlipd(keys=[\"image\", \"mask\"],             # Randomly flips the image and mask for data augmentation.\n",
    "                prob=0.5, spatial_axis=0),        # Flips along the first spatial axis (e.g., horizontal) with a 50% probability.\n",
    "    \n",
    "    RandRotate90d(keys=[\"image\", \"mask\"],         # Randomly rotates the image and mask in 90-degree increments for augmentation.\n",
    "                prob=0.5, max_k=3),               # Applies rotation with a 50% probability, up to 3 times (90, 180, or 270 degrees).\n",
    "    \n",
    "    ToTensord(keys=[\"image\", \"mask\"]),            # Converts the image and mask from NumPy arrays to PyTorch Tensors.\n",
    "])\n",
    "\n",
    "# Define the sequence of transformations for the testing/validation data.\n",
    "# Note: This pipeline does not include random augmentations (like flip or rotate) to ensure consistent evaluation.\n",
    "test_transforms = Compose([ \n",
    "    LoadImaged(keys=[\"image\", \"mask\"]),           # Loads the image and mask data from the file paths specified in the dictionary.\n",
    "    \n",
    "    EnsureChannelFirstd(keys=[\"image\", \"mask\"]),  # Ensures the data format is [Channel, Height, Width].\n",
    "    \n",
    "    ScaleIntensityd(keys=[\"image\"]),              # Normalizes the intensity values of the image.\n",
    "    \n",
    "    Resized(keys=[\"image\", \"mask\"],               # Resize all image to the same shape\n",
    "            spatial_size=target_size),\n",
    "    \n",
    "    ToTensord(keys=[\"image\", \"mask\"]),            # Converts the image and mask from NumPy arrays to PyTorch Tensors.\n",
    "])\n",
    "\n",
    "\n",
    "train_transforms, test_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44b8ce8",
   "metadata": {},
   "source": [
    "## 3. Datasets and Dataloaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c1afbca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count(), os.cpu_count()//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ff6caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create a directory for the persistent cache ---\n",
    "# For best performance, ensure this directory is on a fast drive (like an NVMe SSD).\n",
    "cache_dir = os.path.join(\".\", \"persistent_cache\") # Using a local folder\n",
    "os.makedirs(cache_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "baec4b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create MONAI Datasets and PyTorch DataLoaders ---\n",
    "\n",
    "# Create a training dataset with caching.\n",
    "train_DS = PersistentDataset(\n",
    "    data=train_files,         # The list of file dictionaries for training.\n",
    "    transform=train_transforms, # The transformations (including augmentations) to apply.\n",
    "    cache_dir=os.path.join(cache_dir, \"train\") # Use a sub-directory for the training caches.\n",
    ")\n",
    "\n",
    "# Create a testing dataset. Caching is also beneficial here for faster repeated evaluations.\n",
    "test_DS = PersistentDataset(\n",
    "    data=test_files,          # The list of file dictionaries for testing.\n",
    "    transform=test_transforms,  # The transformations (without augmentations) to apply.\n",
    "    cache_dir=os.path.join(cache_dir, \"test\") # Use a sub-directory for the testing cache\n",
    ")\n",
    "\n",
    "\n",
    "# Create a DataLoader for the training set.\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_DS,         # The dataset to load from.\n",
    "    batch_size=8,             # Number of samples per batch.\n",
    "    shuffle=True,             # Shuffle the data at the beginning of each epoch to improve model generalization.\n",
    "    num_workers=os.cpu_count(),  # Use all available CPU cores to load data in parallel, preventing bottlenecks.\n",
    "    pin_memory=True           # Speeds up data transfer from CPU to GPU by putting tensors in pinned memory.\n",
    ")\n",
    "\n",
    "# Create a DataLoader for the testing set.\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_DS,          # The dataset to load from.\n",
    "    batch_size=4,             # A smaller batch size for evaluation is common.\n",
    "    shuffle=False,            # Do not shuffle the test data to ensure consistent and reproducible evaluation.\n",
    "    num_workers=os.cpu_count()//2, # Use half the CPU cores for loading test data.\n",
    "    pin_memory=True           # Speeds up data transfer from CPU to GPU by putting tensors in pinned memory.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42c0fbf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<monai.data.dataset.PersistentDataset at 0x2b6fea81660>,\n",
       " <monai.data.dataset.PersistentDataset at 0x2b6fea83f40>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x2b66e0bbee0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x2b6fea80a00>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_DS, test_DS, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821ea627",
   "metadata": {},
   "source": [
    "## 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2e1c96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================================================================================================\n",
       "Layer (type:depth-idx)                                                                     Input Shape               Output Shape              Param #                   Trainable\n",
       "==============================================================================================================================================================================================\n",
       "UNet                                                                                       [1, 1, 224, 224]          [1, 1, 224, 224]          --                        True\n",
       "├─Sequential: 1-1                                                                          [1, 1, 224, 224]          [1, 1, 224, 224]          --                        True\n",
       "│    └─ResidualUnit: 2-1                                                                   [1, 1, 224, 224]          [1, 16, 112, 112]         --                        True\n",
       "│    │    └─Conv2d: 3-1                                                                    [1, 1, 224, 224]          [1, 16, 112, 112]         160                       True\n",
       "│    │    └─Sequential: 3-2                                                                [1, 1, 224, 224]          [1, 16, 112, 112]         2,482                     True\n",
       "│    └─SkipConnection: 2-2                                                                 [1, 16, 112, 112]         [1, 32, 112, 112]         --                        True\n",
       "│    │    └─Sequential: 3-3                                                                [1, 16, 112, 112]         [1, 16, 112, 112]         1,621,902                 True\n",
       "│    └─Sequential: 2-3                                                                     [1, 32, 112, 112]         [1, 1, 224, 224]          --                        True\n",
       "│    │    └─Convolution: 3-4                                                               [1, 32, 112, 112]         [1, 1, 224, 224]          290                       True\n",
       "│    │    └─ResidualUnit: 3-5                                                              [1, 1, 224, 224]          [1, 1, 224, 224]          10                        True\n",
       "==============================================================================================================================================================================================\n",
       "Total params: 1,624,844\n",
       "Trainable params: 1,624,844\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 894.06\n",
       "==============================================================================================================================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 29.50\n",
       "Params size (MB): 6.50\n",
       "Estimated Total Size (MB): 36.20\n",
       "=============================================================================================================================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The following code block initializes the U-Net model with specific parameters tailored for this brain tumor segmentation task.\n",
    "\n",
    "U-Net Parameters Explained:\n",
    "---------------------------\n",
    "spatial_dims: Defines the dimensionality of the data. \n",
    "            For 2D MRI slices, this is 2. \n",
    "            For 3D MRI volumes, it would be 3.\n",
    "\n",
    "in_channels: The number of input channels for the model. \n",
    "                For a single grayscale MRI slice, this is 1. \n",
    "                If using multi-modal data (like T1, T1ce, T2, FLAIR from BraTS), this would be 4.\n",
    "\n",
    "out_channels: The number of output channels, which corresponds to the number of classes to be segmented. \n",
    "            For binary segmentation (tumor vs. background), this is 1.\n",
    "\n",
    "channels: A tuple defining the number of feature channels at each level of the U-Net's encoder path. \n",
    "        The model starts with 16 channels, \n",
    "        then downsamples and increases the channel count to 32, 64, 128, and finally 256 at the bottleneck. \n",
    "        The decoder path will mirror this in reverse.\n",
    "\n",
    "strides: A tuple defining the stride for the downsampling convolutions at each level of the encoder. \n",
    "        A stride of 2 halves the spatial dimensions (height and width) of the feature map at each step, which is standard for U-Nets.\n",
    "\n",
    "num_res_units: The number of residual convolutional blocks at each level of the U-Net. \n",
    "            Using 2 units per level increases the model's capacity to learn complex features at different resolutions.\n",
    "\"\"\"\n",
    "\n",
    "# Set the number of classes for segmentation.\n",
    "# For binary segmentation (tumor vs. background), this is 1.\n",
    "num_classes = 1\n",
    "\n",
    "# Set the number of input channels.\n",
    "# Since we are using 2D grayscale MRI slices, this is 1.\n",
    "in_channels = 1\n",
    "\n",
    "# The number of output channels is equal to the number of classes.\n",
    "out_channels = num_classes\n",
    "\n",
    "model = UNet(\n",
    "    spatial_dims=2,                 # Using 2D convolutions and operations.\n",
    "    \n",
    "    in_channels=in_channels,        # Number of channels in the input image.\n",
    "    out_channels=out_channels,      # Number of channels in the output mask.\n",
    "    \n",
    "    channels=(16, 32, 64, 128, 256),# Feature maps at each level of the encoder.\n",
    "    strides=(2, 2, 2, 2),           # Downsampling factor at each encoder level.\n",
    "    \n",
    "    num_res_units=2,                # Number of convolutional blocks per level.\n",
    ").to(device) # Move the model to the specified device (GPU or CPU).\n",
    "\n",
    "\n",
    "\n",
    "summary(\n",
    "    model,\n",
    "    # The input size should match the model's 2D configuration: (batch_size, channels, height, width)\n",
    "    input_size=(1, in_channels, 224, 224),\n",
    "    col_names=(\"input_size\", \"output_size\", \"num_params\", \"trainable\"),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7daace0",
   "metadata": {},
   "source": [
    "## 5. Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d16e3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This block sets up the core components for training the model: \n",
    "the loss function, \n",
    "the optimizer, \n",
    "and a learning rate scheduler.\n",
    "\n",
    "- Loss Function (DiceLoss): Measures how well the model's prediction matches the \n",
    "  ground truth mask. \n",
    "  Dice Loss is particularly effective for segmentation tasks as \n",
    "  it focuses on the overlap between the predicted and actual regions, which is \n",
    "  robust to class imbalance (e.g., small tumors in a large image).\n",
    "\n",
    "- Optimizer (Adam): The algorithm that updates the model's weights to minimize the loss.\n",
    "\n",
    "- Scheduler (ReduceLROnPlateau): A strategy to dynamically adjust the learning \n",
    "  rate during training. It will \"reduce\" the learning rate when the model's \n",
    "  performance on the validation set \"plateaus\" (stops improving). This helps the \n",
    "  model to fine-tune its weights and escape local minima.\n",
    "\"\"\"\n",
    "# Define the loss function. DiceLoss is excellent for segmentation tasks.\n",
    "# sigmoid=True is necessary as our model outputs raw logits. \n",
    "# It applies a sigmoid to the output before calculating loss.\n",
    "loss_fn = DiceLoss(sigmoid=True)\n",
    "\n",
    "# Define the optimizer.\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                            lr=1e-4)\n",
    "\n",
    "# Define a learning rate scheduler. \n",
    "# It will reduce the learning rate when the validation loss stops improving ('min' mode).\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                        mode='min', \n",
    "                                                        factor=0.1, \n",
    "                                                        patience=10, \n",
    "                                                        verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ec4554",
   "metadata": {},
   "source": [
    "## 6. Training loop and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ffe6be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Dice Metric for evaluating segmentation performance.\n",
    "# `include_background=False` means the background class (label 0) is ignored during calculation.\n",
    "# `reduction=\"mean\"` specifies that the Dice scores for all classes will be averaged.\n",
    "metric = DiceMetric(include_background=False,\n",
    "                    reduction= \"mean\")\n",
    "\n",
    "# Set the total number of training epochs.\n",
    "epochs = 20\n",
    "\n",
    "# Initialize a variable to track the best validation Dice score achieved so far.\n",
    "# This is used to save the best performing model checkpoint.\n",
    "best_val_dice = 0.0\n",
    "\n",
    "# Initialize a Gradient Scaler for Automatic Mixed Precision (AMP) training.\n",
    "# This helps prevent underflow of gradients when using float16 precision,\n",
    "# improving training stability and speed on compatible GPUs.\n",
    "scaler = torch.amp.GradScaler(device = \"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85edab9d",
   "metadata": {},
   "source": [
    "### 6.1 Training loop explained in details (don't run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a99480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Start the training loop\n",
    "for epoch in range(epochs):\n",
    "    print (f\"Starting Epoch number = {epoch+1}\")\n",
    "    \n",
    "    # 2. Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # 3. init the total loss for the current epoch\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # 4. Loop over all batches of data in the training loader\n",
    "    for batch in tqdm(train_loader, desc=f\"Train E-{epoch+1}\"):\n",
    "        \n",
    "        # 5. Load images and ground truth masks for the current batch\n",
    "        # and move them to the specified computing device (e.g., \"cuda\" for GPU).\n",
    "        # Move imgs and gts to the GPU asynchronously\n",
    "        imgs = batch[\"image\"].to(device, non_blocking=True)  # Input images (B, C, H, W)\n",
    "        gts = batch[\"mask\"].to(device, non_blocking=True)   # Ground truth masks (B, 1, H, W)\n",
    "    \n",
    "        # 6. Clear the gradients from the previous iteration\n",
    "        # This is essential because PyTorch gradients are accumulated by default.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 7. Use Automatic Mixed Precision (AMP) for training\n",
    "        # This can speed up training and reduce memory usage on compatible hardware (NVIDIA GPUs).\n",
    "        with torch.amp.autocast(device_type=\"cuda\"):\n",
    "            \n",
    "            # 8. Forward pass: Get model predictions (logits) for the input images\n",
    "            logits = model(imgs)  # (B, 1, H, W)\n",
    "            \n",
    "            # 9. Calculate the loss between the model's predictions and the ground truth\n",
    "            loss = loss_fn(logits, gts)\n",
    "            \n",
    "        # 10. Backpropagation: Calculate gradients using the scaled loss\n",
    "        # The GradScaler helps prevent gradients from becoming zero (\"underflowing\") when using float16 precision.\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # 11. Update model weights\n",
    "        # The scaler checks for invalid gradients (NaNs/Infs) and tells the optimizer to update the weights.\n",
    "        scaler.step(optimizer)\n",
    "        \n",
    "        # 12. Update the GradScaler's scale factor for the next iteration\n",
    "        scaler.update()\n",
    "        \n",
    "        # 13. Accumulate the loss for the current batch\n",
    "        # .item() gets the scalar value from the loss tensor.\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # 14. Calculate the average loss for the entire epoch\n",
    "    epoch_loss /= len(train_loader)\n",
    "    \n",
    "    # 15. Save the model checkpoint after the epoch\n",
    "    checkpoint_path = os.path.join(\"checkpoints/\", f\"model_epoch_{epoch+1}.pth\")\n",
    "    # Saving the model's state_dict is the recommended practice for flexibility.\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1} Average Loss: {epoch_loss:.4f}\")\n",
    "    print(f\"Checkpoint saved: {checkpoint_path}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c619c8d",
   "metadata": {},
   "source": [
    "### 6.2 Testing loop explained in details (don't run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579477b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Start Validation Phase ---\n",
    "\n",
    "# 1. Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# 2. Initialize a list to store the average Dice score for each batch\n",
    "val_dice_scores = []\n",
    "\n",
    "# 3. Use torch.inference_mode() to disable gradient calculations\n",
    "with torch.inference_mode():\n",
    "    \n",
    "    # 4. Loop over all batches in the test_loader\n",
    "    for batch_data in tqdm(test_loader, desc=f\"Validation E-{epoch+1}\"):\n",
    "        \n",
    "        # 5. Load images and masks for the current batch and move to the GPU\n",
    "        #    - `non_blocking=True` allows for asynchronous data transfer, which can\n",
    "        #      overlap with other computations and speed up the pipeline.\n",
    "        test_img = batch_data[\"image\"].to(device, non_blocking=True)\n",
    "        test_gts = batch_data[\"mask\"].to(device, non_blocking=True)\n",
    "        \n",
    "        # 6. Use Automatic Mixed Precision (AMP) for the forward pass\n",
    "        #    - `autocast` automatically uses float16 for eligible operations, which\n",
    "        #      speeds up inference and reduces GPU memory usage on compatible hardware.\n",
    "        with torch.amp.autocast(device_type=\"cuda\"):\n",
    "            # Get the raw model outputs (logits)\n",
    "            test_outputs = model(test_img)\n",
    "            \n",
    "            # 7. Convert logits to binary predictions\n",
    "            #    - Apply the sigmoid function to squash outputs to a [0, 1] range (probabilities).\n",
    "            #    - Threshold the probabilities at 0.5 to create a binary mask (0 or 1).\n",
    "            #    - Convert the boolean mask to a float tensor for calculations.\n",
    "            preds = (torch.sigmoid(test_outputs) > 0.5).float()\n",
    "            \n",
    "        # 8. Calculate the Dice score for each image in the batch\n",
    "        #    - The Dice coefficient is a standard metric for segmentation tasks.\n",
    "        #    - Formula: (2 * |A ∩ B|) / (|A| + |B|)\n",
    "        #    - We sum over the channel, height, and width dimensions (1, 2, 3).\n",
    "        #    - `1e-6` (epsilon) is added for numerical stability to avoid division by zero.\n",
    "        intersection = (preds * test_gts).sum(dim=(1, 2, 3))\n",
    "        union = preds.sum(dim=(1, 2, 3)) + test_gts.sum(dim=(1, 2, 3))\n",
    "        dice_per_image = (2.0 * intersection + 1e-6) / (union + 1e-6)\n",
    "        \n",
    "        # 9. Append the mean Dice score of the batch to our list of scores\n",
    "        #    - We take the mean of the Dice scores calculated for each image in the batch.\n",
    "        #    - `.item()` extracts the scalar value from the tensor.\n",
    "        val_dice_scores.append(dice_per_image.mean().item())\n",
    "\n",
    "# 10. Calculate the final validation Dice score for the entire epoch\n",
    "#     - This is the average of the mean Dice scores from all batches.\n",
    "mean_val_dice = sum(val_dice_scores) / len(val_dice_scores)\n",
    "\n",
    "# 11. Update the learning rate scheduler\n",
    "#     - For schedulers like ReduceLROnPlateau, this adjusts the LR based on the\n",
    "#       validation metric. We use (1.0 - Dice) because schedulers typically\n",
    "#       monitor a value that should be minimized (like loss).\n",
    "if scheduler:\n",
    "    scheduler.step(1.0 - mean_val_dice)\n",
    "\n",
    "# 12. Print a summary of the epoch's performance\n",
    "print(f\"\\n--- Epoch {epoch+1} Summary ---\")\n",
    "print(f\"  Train Loss: {epoch_loss:.4f}\")\n",
    "print(f\"  Validation Dice: {mean_val_dice:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0242037a",
   "metadata": {},
   "source": [
    "### 6.3 Make them into functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4dd0cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# 1. DEFINE THE TRAINING FUNCTION FOR ONE EPOCH\n",
    "# ======================================================================================\n",
    "def train_one_epoch(model, loader, optimizer, loss_fn, scaler, device, epoch_num):\n",
    "    \"\"\"\n",
    "    Performs one full training epoch.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to be trained.\n",
    "        loader (DataLoader): The DataLoader for the training data.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer for updating weights.\n",
    "        loss_fn: The loss function.\n",
    "        scaler (torch.cuda.amp.GradScaler): The gradient scaler for mixed precision.\n",
    "        device (torch.device): The device to train on (e.g., \"cuda\").\n",
    "        epoch_num (int): The current epoch number, for logging.\n",
    "        \n",
    "    Returns:\n",
    "        float: The average training loss for the epoch.\n",
    "    \"\"\"\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    progress_bar = tqdm(loader, desc=f\"Train E-{epoch_num}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Move data to the target device asynchronously\n",
    "        imgs = batch[\"image\"].to(device, non_blocking=True)\n",
    "        gts = batch[\"mask\"].to(device, non_blocking=True)\n",
    "        \n",
    "        # Clear previous gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass with Automatic Mixed Precision (AMP)\n",
    "        with torch.amp.autocast(device_type=\"cuda\"):\n",
    "            logits = model(imgs)\n",
    "            loss = loss_fn(logits, gts)\n",
    "            \n",
    "        # Backward pass and optimization\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Accumulate loss and update progress bar\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "        \n",
    "    # Return the average loss for the epoch\n",
    "    return epoch_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c57688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, device, epoch_num):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the validation set.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to be evaluated.\n",
    "        loader (DataLoader): The DataLoader for the validation data.\n",
    "        device (torch.device): The device to evaluate on.\n",
    "        epoch_num (int): The current epoch number, for logging.\n",
    "        \n",
    "    Returns:\n",
    "        tuple[float, float]: A tuple containing the average validation Dice score\n",
    "                             and the average validation accuracy.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    val_dice_scores = []\n",
    "    val_acc = []\n",
    "    progress_bar = tqdm(loader, desc=f\"Validation E-{epoch_num}\")\n",
    "    \n",
    "    # Disable gradient calculations for efficiency\n",
    "    with torch.inference_mode():\n",
    "        for batch_data in progress_bar:\n",
    "            # Move data to the target device\n",
    "            test_img = batch_data[\"image\"].to(device, non_blocking=True)\n",
    "            test_gts = batch_data[\"mask\"].to(device, non_blocking=True)\n",
    "            \n",
    "            # Forward pass with AMP\n",
    "            with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                test_outputs = model(test_img)\n",
    "                preds = (torch.sigmoid(test_outputs) > 0.5).float()\n",
    "                \n",
    "            # Calculate Dice score for the batch\n",
    "            intersection = (preds * test_gts).sum(dim=(1, 2, 3))\n",
    "            union = preds.sum(dim=(1, 2, 3)) + test_gts.sum(dim=(1, 2, 3))\n",
    "            dice_per_image = (2.0 * intersection + 1e-6) / (union + 1e-6)\n",
    "            batch_dice = dice_per_image.mean().item()\n",
    "            \n",
    "            # Calculate pixel-wise accuracy for the batch\n",
    "            # - Accuracy = (Correctly classified pixels) / (Total pixels)\n",
    "            correct_pixels = (preds == test_gts).sum()\n",
    "            total_pixels = test_gts.numel() # Total number of pixels in the batch\n",
    "            batch_accuracy = (correct_pixels / total_pixels).item()\n",
    "            \n",
    "            # Store the mean dice score for the batch\n",
    "            val_dice_scores.append(batch_dice)\n",
    "            val_acc.append(batch_accuracy)\n",
    "            progress_bar.set_postfix(dice=batch_dice, acc=batch_accuracy)\n",
    "            \n",
    "    # Return the average Dice and Accuracy scores across all validation batches\n",
    "    mean_dice = sum(val_dice_scores) / len(val_dice_scores)\n",
    "    mean_accuracy = sum(val_acc) / len(val_acc)\n",
    "    return mean_dice, mean_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d47448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Starting Epoch 1/20 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E-1: 100%|██████████| 307/307 [03:14<00:00,  1.58it/s, loss=-0.871]  \n",
      "Validation E-1: 100%|██████████| 154/154 [00:32<00:00,  4.72it/s, acc=0.861, dice=1.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1 Summary ---\n",
      "  Train Loss:      -0.8256\n",
      "  Validation Dice:   1.8485\n",
      "  Validation Acc:    0.8622\n",
      "  Checkpoint saved:  checkpoints/model_epoch_1.pth\n",
      "============================\n",
      "\n",
      "===== Starting Epoch 2/20 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E-2:   0%|          | 0/307 [00:19<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m===== Starting Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m =====\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Run the training function\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Run the evaluation function\u001b[39;00m\n\u001b[0;32m     14\u001b[0m val_dice, val_acc \u001b[38;5;241m=\u001b[39m evaluate(model, \n\u001b[0;32m     15\u001b[0m                             test_loader, \n\u001b[0;32m     16\u001b[0m                             device, \n\u001b[0;32m     17\u001b[0m                             epoch)\n",
      "Cell \u001b[1;32mIn[23], line 26\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, loader, optimizer, loss_fn, scaler, device, epoch_num)\u001b[0m\n\u001b[0;32m     23\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     24\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain E-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# Move data to the target device asynchronously\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     imgs \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     29\u001b[0m     gts \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Codess & Projects\\.venv\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Codess & Projects\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:491\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Codess & Projects\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:422\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Codess & Projects\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1139\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1146\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\context.py:327\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    print(f\"\\n===== Starting Epoch {epoch}/{epochs} =====\")\n",
    "    \n",
    "    # Run the training function\n",
    "    train_loss = train_one_epoch(model,\n",
    "                                train_loader, \n",
    "                                optimizer, \n",
    "                                loss_fn, \n",
    "                                scaler, \n",
    "                                device, \n",
    "                                epoch)\n",
    "    \n",
    "    # Run the evaluation function\n",
    "    val_dice, val_acc = evaluate(model, \n",
    "                                test_loader, \n",
    "                                device, \n",
    "                                epoch)\n",
    "    \n",
    "    # Update the learning rate scheduler based on the validation metric\n",
    "    if scheduler:\n",
    "        # We use (1.0 - Dice) because schedulers typically monitor a value to be minimized\n",
    "        scheduler.step(1.0 - val_dice)\n",
    "        \n",
    "    # Save the model checkpoint\n",
    "    checkpoint_path = os.path.join(\"checkpoints/\", f\"model_epoch_{epoch}.pth\")\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    \n",
    "    # Print a summary of the epoch's performance\n",
    "    print(f\"\\n--- Epoch {epoch} Summary ---\")\n",
    "    print(f\"  Train Loss:      {train_loss:.4f}\")\n",
    "    print(f\"  Validation Dice:   {val_dice:.4f}\")\n",
    "    print(f\"  Validation Acc:    {val_acc*100:.4f}\")\n",
    "    print(f\"  Checkpoint saved:  {checkpoint_path}\")\n",
    "    print(\"=\" * (35 + len(str(epoch)) + len(str(epochs))))\n",
    "\n",
    "print(\"\\nTraining finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcad2779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
