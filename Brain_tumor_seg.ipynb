{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c29688d4",
   "metadata": {},
   "source": [
    "# Brain tumor segmentation model using MONAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c30c05",
   "metadata": {},
   "source": [
    "## Quick overview of steps:\n",
    "\n",
    "1. Build file lists (images, masks).\n",
    "1. Build MONAI transforms for images & masks.\n",
    "1. Create Dataset / Dataloader (CacheDataset for speed).\n",
    "1. Create model (MONAI UNet for 2D).\n",
    "1. Load pretrained checkpoint (if available) into model.\n",
    "1. Define loss, optimizer, scheduler.\n",
    "1. Train (with validation, metrics, checkpointing).\n",
    "1. Fine-tuning tips (freeze, low LR, augmentations, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fc18a1",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37d58d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda'), '2.6.0+cu126')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "\n",
    "from monai.transforms import (\n",
    "    LoadImaged, EnsureChannelFirstd, ScaleIntensityd, ToTensord,\n",
    "    RandFlipd, RandRotate90d, Compose\n",
    ")\n",
    "from monai.data import Dataset, CacheDataset\n",
    "from monai.networks.nets import UNet\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "set_determinism(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device, torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2585f80f",
   "metadata": {},
   "source": [
    "## 1. file lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bde988f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from training set: {'image': WindowsPath('brain_tumor_dataset/images/1041.png'), 'mask': WindowsPath('brain_tumor_dataset/masks/1041.png')}\n",
      "Sample from testing set: {'image': WindowsPath('brain_tumor_dataset/images/1284.png'), 'mask': WindowsPath('brain_tumor_dataset/masks/1284.png')}\n",
      "Length of training set: 2451\n",
      "Length of testing set: 613\n"
     ]
    }
   ],
   "source": [
    "# Define paths to the image and mask directories\n",
    "IMAGE_DIR = Path(\"brain_tumor_dataset/images\")\n",
    "MASK_DIR = Path(\"brain_tumor_dataset/masks\")\n",
    "\n",
    "# Get all image and mask file paths, assuming they are PNG files\n",
    "images = list(IMAGE_DIR.glob(\"*.png\"))\n",
    "masks = list(MASK_DIR.glob(\"*.png\"))\n",
    "\n",
    "# Create a list of dictionaries, pairing each image with its corresponding mask.\n",
    "# This assumes that the files in both directories are sorted in a corresponding order.\n",
    "data_dicts = [\n",
    "    {\n",
    "        \"image\": img, \n",
    "        \"mask\": m\n",
    "        \n",
    "    } for img, m in zip(images, masks)\n",
    "]\n",
    "\n",
    "# --- Data Splitting ---\n",
    "import random\n",
    "# Set a random seed for reproducibility, ensuring the shuffle is the same every time.\n",
    "random.seed(42) \n",
    "# Shuffle the data dictionaries in place to randomize the dataset.\n",
    "random.shuffle(data_dicts)\n",
    "\n",
    "# Calculate the split index for an 80/20 train-test split.\n",
    "n_val = int(0.8 * len(data_dicts)) \n",
    "# Split the data into training and testing sets.\n",
    "train_files = data_dicts[:n_val]\n",
    "test_files = data_dicts[n_val:]\n",
    "\n",
    "# Print a sample from each set to verify the structure.\n",
    "print(\"Sample from training set:\", train_files[5])\n",
    "print(\"Sample from testing set:\", test_files[5])\n",
    "print (f\"Length of training set: {len(train_files)}\")\n",
    "print (f\"Length of testing set: {len(test_files)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84c2d98",
   "metadata": {},
   "source": [
    "## 2. transforms 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8712815c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<monai.transforms.compose.Compose at 0x2cb7f329fc0>,\n",
       " <monai.transforms.compose.Compose at 0x2cb7fa7c5e0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the sequence of transformations for the training data. These include data augmentation steps.\n",
    "train_transforms = Compose([ \n",
    "    LoadImaged(keys=[\"image\", \"mask\"]),           # Loads the image and mask data from the file paths specified in the dictionary.\n",
    "    \n",
    "    EnsureChannelFirstd(keys=[\"image\", \"mask\"]),  # Ensures the data format is [Channel, Height, Width], which is standard for PyTorch.\n",
    "    \n",
    "    ScaleIntensityd(keys=[\"image\"]),              # Normalizes the intensity values of the image (e.g., to a [0, 1] range). \n",
    "    \n",
    "    RandFlipd(keys=[\"image\", \"mask\"],             # Randomly flips the image and mask for data augmentation.\n",
    "                prob=0.5, spatial_axis=0),        # Flips along the first spatial axis (e.g., horizontal) with a 50% probability.\n",
    "    \n",
    "    RandRotate90d(keys=[\"image\", \"mask\"],         # Randomly rotates the image and mask in 90-degree increments for augmentation.\n",
    "                prob=0.5, max_k=3),               # Applies rotation with a 50% probability, up to 3 times (90, 180, or 270 degrees).\n",
    "    \n",
    "    ToTensord(keys=[\"image\", \"mask\"]),            # Converts the image and mask from NumPy arrays to PyTorch Tensors.\n",
    "])\n",
    "\n",
    "# Define the sequence of transformations for the testing/validation data.\n",
    "# Note: This pipeline does not include random augmentations (like flip or rotate) to ensure consistent evaluation.\n",
    "test_transforms = Compose([ \n",
    "    LoadImaged(keys=[\"image\", \"mask\"]),           # Loads the image and mask data from the file paths specified in the dictionary.\n",
    "    \n",
    "    EnsureChannelFirstd(keys=[\"image\", \"mask\"]),  # Ensures the data format is [Channel, Height, Width].\n",
    "    \n",
    "    ScaleIntensityd(keys=[\"image\"]),              # Normalizes the intensity values of the image.\n",
    "    \n",
    "    ToTensord(keys=[\"image\", \"mask\"]),            # Converts the image and mask from NumPy arrays to PyTorch Tensors.\n",
    "])\n",
    "\n",
    "\n",
    "train_transforms, test_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44b8ce8",
   "metadata": {},
   "source": [
    "## 3. Datasets and Dataloaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c1afbca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count(), os.cpu_count()//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baec4b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset:   0%|          | 0/2451 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 2451/2451 [00:33<00:00, 74.21it/s] \n",
      "Loading dataset: 100%|██████████| 613/613 [00:08<00:00, 73.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- Create MONAI Datasets and PyTorch DataLoaders ---\n",
    "\n",
    "# Create a training dataset with caching.\n",
    "train_DS = CacheDataset(\n",
    "    data=train_files,         # The list of file dictionaries for training.\n",
    "    transform=train_transforms, # The transformations (including augmentations) to apply.\n",
    "    cache_rate=1.0            # Cache 100% of the transformed data in RAM for fast access during training epochs.\n",
    ")\n",
    "\n",
    "# Create a testing dataset. Caching is also beneficial here for faster repeated evaluations.\n",
    "test_DS = CacheDataset(\n",
    "    data=test_files,          # The list of file dictionaries for testing.\n",
    "    transform=test_transforms,  # The transformations (without augmentations) to apply.\n",
    "    cache_rate=1.0            # Suggestion: Caching the test set speeds up evaluation.\n",
    ")\n",
    "\n",
    "\n",
    "# Create a DataLoader for the training set.\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_DS,         # The dataset to load from.\n",
    "    batch_size=8,             # Number of samples per batch.\n",
    "    shuffle=True,             # Shuffle the data at the beginning of each epoch to improve model generalization.\n",
    "    num_workers=os.cpu_count()  # Use all available CPU cores to load data in parallel, preventing bottlenecks.\n",
    ")\n",
    "\n",
    "# Create a DataLoader for the testing set.\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_DS,          # The dataset to load from.\n",
    "    batch_size=4,             # A smaller batch size for evaluation is common.\n",
    "    shuffle=False,            # Do not shuffle the test data to ensure consistent and reproducible evaluation.\n",
    "    num_workers=os.cpu_count()//2 # Use half the CPU cores for loading test data.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42c0fbf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<monai.data.dataset.CacheDataset at 0x2cb072bf700>,\n",
       " <monai.data.dataset.CacheDataset at 0x2cb556eeef0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x2cb274187f0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x2cb7fdfeef0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_DS, test_DS, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821ea627",
   "metadata": {},
   "source": [
    "## 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2e1c96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================================================================================================\n",
       "Layer (type:depth-idx)                                                                     Input Shape               Output Shape              Param #                   Trainable\n",
       "==============================================================================================================================================================================================\n",
       "UNet                                                                                       [1, 1, 224, 224]          [1, 1, 224, 224]          --                        True\n",
       "├─Sequential: 1-1                                                                          [1, 1, 224, 224]          [1, 1, 224, 224]          --                        True\n",
       "│    └─ResidualUnit: 2-1                                                                   [1, 1, 224, 224]          [1, 16, 112, 112]         --                        True\n",
       "│    │    └─Conv2d: 3-1                                                                    [1, 1, 224, 224]          [1, 16, 112, 112]         160                       True\n",
       "│    │    └─Sequential: 3-2                                                                [1, 1, 224, 224]          [1, 16, 112, 112]         2,482                     True\n",
       "│    └─SkipConnection: 2-2                                                                 [1, 16, 112, 112]         [1, 32, 112, 112]         --                        True\n",
       "│    │    └─Sequential: 3-3                                                                [1, 16, 112, 112]         [1, 16, 112, 112]         1,621,902                 True\n",
       "│    └─Sequential: 2-3                                                                     [1, 32, 112, 112]         [1, 1, 224, 224]          --                        True\n",
       "│    │    └─Convolution: 3-4                                                               [1, 32, 112, 112]         [1, 1, 224, 224]          290                       True\n",
       "│    │    └─ResidualUnit: 3-5                                                              [1, 1, 224, 224]          [1, 1, 224, 224]          10                        True\n",
       "==============================================================================================================================================================================================\n",
       "Total params: 1,624,844\n",
       "Trainable params: 1,624,844\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 894.06\n",
       "==============================================================================================================================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 29.50\n",
       "Params size (MB): 6.50\n",
       "Estimated Total Size (MB): 36.20\n",
       "=============================================================================================================================================================================================="
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The following code block initializes the U-Net model with specific parameters tailored for this brain tumor segmentation task.\n",
    "\n",
    "U-Net Parameters Explained:\n",
    "---------------------------\n",
    "spatial_dims: Defines the dimensionality of the data. \n",
    "            For 2D MRI slices, this is 2. \n",
    "            For 3D MRI volumes, it would be 3.\n",
    "\n",
    "in_channels: The number of input channels for the model. \n",
    "                For a single grayscale MRI slice, this is 1. \n",
    "                If using multi-modal data (like T1, T1ce, T2, FLAIR from BraTS), this would be 4.\n",
    "\n",
    "out_channels: The number of output channels, which corresponds to the number of classes to be segmented. \n",
    "            For binary segmentation (tumor vs. background), this is 1.\n",
    "\n",
    "channels: A tuple defining the number of feature channels at each level of the U-Net's encoder path. \n",
    "        The model starts with 16 channels, \n",
    "        then downsamples and increases the channel count to 32, 64, 128, and finally 256 at the bottleneck. \n",
    "        The decoder path will mirror this in reverse.\n",
    "\n",
    "strides: A tuple defining the stride for the downsampling convolutions at each level of the encoder. \n",
    "        A stride of 2 halves the spatial dimensions (height and width) of the feature map at each step, which is standard for U-Nets.\n",
    "\n",
    "num_res_units: The number of residual convolutional blocks at each level of the U-Net. \n",
    "            Using 2 units per level increases the model's capacity to learn complex features at different resolutions.\n",
    "\"\"\"\n",
    "\n",
    "# Set the number of classes for segmentation.\n",
    "# For binary segmentation (tumor vs. background), this is 1.\n",
    "num_classes = 1\n",
    "\n",
    "# Set the number of input channels.\n",
    "# Since we are using 2D grayscale MRI slices, this is 1.\n",
    "in_channels = 1\n",
    "\n",
    "# The number of output channels is equal to the number of classes.\n",
    "out_channels = num_classes\n",
    "\n",
    "model = UNet(\n",
    "    spatial_dims=2,                 # Using 2D convolutions and operations.\n",
    "    \n",
    "    in_channels=in_channels,        # Number of channels in the input image.\n",
    "    out_channels=out_channels,      # Number of channels in the output mask.\n",
    "    \n",
    "    channels=(16, 32, 64, 128, 256),# Feature maps at each level of the encoder.\n",
    "    strides=(2, 2, 2, 2),           # Downsampling factor at each encoder level.\n",
    "    \n",
    "    num_res_units=2,                # Number of convolutional blocks per level.\n",
    ").to(device) # Move the model to the specified device (GPU or CPU).\n",
    "\n",
    "\n",
    "\n",
    "summary(\n",
    "    model,\n",
    "    # The input size should match the model's 2D configuration: (batch_size, channels, height, width)\n",
    "    input_size=(1, in_channels, 224, 224),\n",
    "    col_names=(\"input_size\", \"output_size\", \"num_params\", \"trainable\"),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7daace0",
   "metadata": {},
   "source": [
    "# 5. Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d16e3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This block sets up the core components for training the model: \n",
    "the loss function, \n",
    "the optimizer, \n",
    "and a learning rate scheduler.\n",
    "\n",
    "- Loss Function (DiceLoss): Measures how well the model's prediction matches the \n",
    "  ground truth mask. \n",
    "  Dice Loss is particularly effective for segmentation tasks as \n",
    "  it focuses on the overlap between the predicted and actual regions, which is \n",
    "  robust to class imbalance (e.g., small tumors in a large image).\n",
    "\n",
    "- Optimizer (Adam): The algorithm that updates the model's weights to minimize the loss.\n",
    "\n",
    "- Scheduler (ReduceLROnPlateau): A strategy to dynamically adjust the learning \n",
    "  rate during training. It will \"reduce\" the learning rate when the model's \n",
    "  performance on the validation set \"plateaus\" (stops improving). This helps the \n",
    "  model to fine-tune its weights and escape local minima.\n",
    "\"\"\"\n",
    "# Define the loss function. DiceLoss is excellent for segmentation tasks.\n",
    "# sigmoid=True is necessary as our model outputs raw logits. \n",
    "# It applies a sigmoid to the output before calculating loss.\n",
    "loss_fn = DiceLoss(sigmoid=True)\n",
    "\n",
    "# Define the optimizer.\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                            lr=1e-4)\n",
    "\n",
    "# Define a learning rate scheduler. \n",
    "# It will reduce the learning rate when the validation loss stops improving ('min' mode).\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                        mode='min', \n",
    "                                                        factor=0.1, \n",
    "                                                        patience=10, \n",
    "                                                        verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffe6be2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
