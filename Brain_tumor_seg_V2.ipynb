{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72f5ee8b",
   "metadata": {},
   "source": [
    "# Brain Tumor Segmentation — DeepLabV3 Transfer Learning (v2)\n",
    "\n",
    "This notebook fine-tunes a pretrained DeepLabV3 (ResNet-50) model from torchvision for binary brain tumor segmentation on single-channel (grayscale) MRI slices.\n",
    "\n",
    "Primary goals:\n",
    "\n",
    "- Adapt a pretrained DeepLabV3 model to accept 1-channel input and output two classes (background vs tumor).\n",
    "- Freeze the ResNet backbone and fine-tune the segmentation heads to leverage pretrained features while reducing training time.\n",
    "- Create a robust PyTorch Dataset and DataLoaders for grayscale images and binary masks.\n",
    "- Use Dice loss and MONAI's Dice metric for segmentation-specific optimization and evaluation. Automatic Mixed Precision (AMP) is enabled for faster GPU training.\n",
    "\n",
    "Assumptions:\n",
    "\n",
    "- Input images are stored as PNG files in `brain_tumor_dataset/images` and masks in `brain_tumor_dataset/masks`.\n",
    "- Masks are binary or use 0/255; the dataset class binarizes masks automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab811a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda'), '2.8.0+cu126', '0.23.0+cu126')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device, torch.__version__, torchvision.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6d2e1b",
   "metadata": {},
   "source": [
    "## 1. Load the pretrained model\n",
    "\n",
    "We load DeepLabV3 (ResNet-50) with torchvision's DEFAULT weights to leverage pretrained feature extractors. The weights object provides a `transforms()` helper describing the original model's expected preprocessing.\n",
    "\n",
    "Because our MRI slices are grayscale, we build a custom transform pipeline that:\n",
    "\n",
    "- Resizes images to the network's expected resolution,\n",
    "- Converts RGB to a single grayscale channel,\n",
    "- Converts to a tensor and normalizes with a 1-channel mean/std.\n",
    "\n",
    "After loading the pretrained model we will:\n",
    "\n",
    "1. Freeze the backbone parameters to keep pretrained features fixed during initial training.\n",
    "2. Replace the classifier and auxiliary classifier to output 2 channels (background vs tumor).\n",
    "3. Update the first convolution to accept a single input channel.\n",
    "\n",
    "This approach reduces the amount of training required while adapting the architecture to our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b47f91ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepLabV3(\n",
       "  (backbone): IntermediateLayerGetter(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): DeepLabHead(\n",
       "    (0): ASPP(\n",
       "      (convs): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ASPPConv(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): ASPPConv(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (3): ASPPConv(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(36, 36), dilation=(36, 36), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (4): ASPPPooling(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (project): Sequential(\n",
       "        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (aux_classifier): FCNHead(\n",
       "    (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torchvision.models.segmentation.DeepLabV3_ResNet50_Weights.DEFAULT\n",
    "transforms = weights.transforms()\n",
    "\n",
    "deeplap_V3 = torchvision.models.segmentation.deeplabv3_resnet50(weights = weights)\n",
    "deeplap_V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03f3b258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "======================================================================================================================================================\n",
       "Layer (type:depth-idx)                             Input Shape               Output Shape              Param #                   Trainable\n",
       "======================================================================================================================================================\n",
       "DeepLabV3                                          [1, 3, 224, 224]          [1, 21, 224, 224]         --                        True\n",
       "├─IntermediateLayerGetter: 1-1                     [1, 3, 224, 224]          [1, 2048, 28, 28]         --                        True\n",
       "│    └─Conv2d: 2-1                                 [1, 3, 224, 224]          [1, 64, 112, 112]         9,408                     True\n",
       "│    └─BatchNorm2d: 2-2                            [1, 64, 112, 112]         [1, 64, 112, 112]         128                       True\n",
       "│    └─ReLU: 2-3                                   [1, 64, 112, 112]         [1, 64, 112, 112]         --                        --\n",
       "│    └─MaxPool2d: 2-4                              [1, 64, 112, 112]         [1, 64, 56, 56]           --                        --\n",
       "│    └─Sequential: 2-5                             [1, 64, 56, 56]           [1, 256, 56, 56]          --                        True\n",
       "│    │    └─Bottleneck: 3-1                        [1, 64, 56, 56]           [1, 256, 56, 56]          75,008                    True\n",
       "│    │    └─Bottleneck: 3-2                        [1, 256, 56, 56]          [1, 256, 56, 56]          70,400                    True\n",
       "│    │    └─Bottleneck: 3-3                        [1, 256, 56, 56]          [1, 256, 56, 56]          70,400                    True\n",
       "│    └─Sequential: 2-6                             [1, 256, 56, 56]          [1, 512, 28, 28]          --                        True\n",
       "│    │    └─Bottleneck: 3-4                        [1, 256, 56, 56]          [1, 512, 28, 28]          379,392                   True\n",
       "│    │    └─Bottleneck: 3-5                        [1, 512, 28, 28]          [1, 512, 28, 28]          280,064                   True\n",
       "│    │    └─Bottleneck: 3-6                        [1, 512, 28, 28]          [1, 512, 28, 28]          280,064                   True\n",
       "│    │    └─Bottleneck: 3-7                        [1, 512, 28, 28]          [1, 512, 28, 28]          280,064                   True\n",
       "│    └─Sequential: 2-7                             [1, 512, 28, 28]          [1, 1024, 28, 28]         --                        True\n",
       "│    │    └─Bottleneck: 3-8                        [1, 512, 28, 28]          [1, 1024, 28, 28]         1,512,448                 True\n",
       "│    │    └─Bottleneck: 3-9                        [1, 1024, 28, 28]         [1, 1024, 28, 28]         1,117,184                 True\n",
       "│    │    └─Bottleneck: 3-10                       [1, 1024, 28, 28]         [1, 1024, 28, 28]         1,117,184                 True\n",
       "│    │    └─Bottleneck: 3-11                       [1, 1024, 28, 28]         [1, 1024, 28, 28]         1,117,184                 True\n",
       "│    │    └─Bottleneck: 3-12                       [1, 1024, 28, 28]         [1, 1024, 28, 28]         1,117,184                 True\n",
       "│    │    └─Bottleneck: 3-13                       [1, 1024, 28, 28]         [1, 1024, 28, 28]         1,117,184                 True\n",
       "│    └─Sequential: 2-8                             [1, 1024, 28, 28]         [1, 2048, 28, 28]         --                        True\n",
       "│    │    └─Bottleneck: 3-14                       [1, 1024, 28, 28]         [1, 2048, 28, 28]         6,039,552                 True\n",
       "│    │    └─Bottleneck: 3-15                       [1, 2048, 28, 28]         [1, 2048, 28, 28]         4,462,592                 True\n",
       "│    │    └─Bottleneck: 3-16                       [1, 2048, 28, 28]         [1, 2048, 28, 28]         4,462,592                 True\n",
       "├─DeepLabHead: 1-2                                 [1, 2048, 28, 28]         [1, 21, 28, 28]           --                        True\n",
       "│    └─ASPP: 2-9                                   [1, 2048, 28, 28]         [1, 256, 28, 28]          --                        True\n",
       "│    │    └─ModuleList: 3-17                       --                        --                        15,206,912                True\n",
       "│    │    └─Sequential: 3-18                       [1, 1280, 28, 28]         [1, 256, 28, 28]          328,192                   True\n",
       "│    └─Conv2d: 2-10                                [1, 256, 28, 28]          [1, 256, 28, 28]          589,824                   True\n",
       "│    └─BatchNorm2d: 2-11                           [1, 256, 28, 28]          [1, 256, 28, 28]          512                       True\n",
       "│    └─ReLU: 2-12                                  [1, 256, 28, 28]          [1, 256, 28, 28]          --                        --\n",
       "│    └─Conv2d: 2-13                                [1, 256, 28, 28]          [1, 21, 28, 28]           5,397                     True\n",
       "├─FCNHead: 1-3                                     [1, 1024, 28, 28]         [1, 21, 28, 28]           --                        True\n",
       "│    └─Conv2d: 2-14                                [1, 1024, 28, 28]         [1, 256, 28, 28]          2,359,296                 True\n",
       "│    └─BatchNorm2d: 2-15                           [1, 256, 28, 28]          [1, 256, 28, 28]          512                       True\n",
       "│    └─ReLU: 2-16                                  [1, 256, 28, 28]          [1, 256, 28, 28]          --                        --\n",
       "│    └─Dropout: 2-17                               [1, 256, 28, 28]          [1, 256, 28, 28]          --                        --\n",
       "│    └─Conv2d: 2-18                                [1, 256, 28, 28]          [1, 21, 28, 28]           5,397                     True\n",
       "======================================================================================================================================================\n",
       "Total params: 42,004,074\n",
       "Trainable params: 42,004,074\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 33.16\n",
       "======================================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 425.76\n",
       "Params size (MB): 168.02\n",
       "Estimated Total Size (MB): 594.38\n",
       "======================================================================================================================================================"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(\n",
    "    deeplap_V3,\n",
    "    # The input size should match the model's 2D configuration: (batch_size, channels, height, width)\n",
    "    input_size=(1, 3, 224, 224),\n",
    "    col_names=(\"input_size\", \"output_size\", \"num_params\", \"trainable\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d51404ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SemanticSegmentation(\n",
       "    resize_size=[520]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BILINEAR\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms # notice how the mean and std are on 3 channels, but we need only one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab59b5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transform for single-channel (grayscale) images\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "deeplab_V3_transform = T.Compose([\n",
    "    T.Resize(520, interpolation=InterpolationMode.BILINEAR),\n",
    "    T.Grayscale(num_output_channels=1),   # convert to 1 channel\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5], std=[0.5])    # adjust mean/std for 1 channel\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccbe0175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "======================================================================================================================================================\n",
       "Layer (type:depth-idx)                             Input Shape               Output Shape              Param #                   Trainable\n",
       "======================================================================================================================================================\n",
       "DeepLabV3                                          [1, 1, 224, 224]          [1, 2, 224, 224]          --                        Partial\n",
       "├─IntermediateLayerGetter: 1-1                     [1, 1, 224, 224]          [1, 2048, 28, 28]         --                        Partial\n",
       "│    └─Conv2d: 2-1                                 [1, 1, 224, 224]          [1, 64, 112, 112]         3,136                     True\n",
       "│    └─BatchNorm2d: 2-2                            [1, 64, 112, 112]         [1, 64, 112, 112]         (128)                     False\n",
       "│    └─ReLU: 2-3                                   [1, 64, 112, 112]         [1, 64, 112, 112]         --                        --\n",
       "│    └─MaxPool2d: 2-4                              [1, 64, 112, 112]         [1, 64, 56, 56]           --                        --\n",
       "│    └─Sequential: 2-5                             [1, 64, 56, 56]           [1, 256, 56, 56]          --                        False\n",
       "│    │    └─Bottleneck: 3-1                        [1, 64, 56, 56]           [1, 256, 56, 56]          (75,008)                  False\n",
       "│    │    └─Bottleneck: 3-2                        [1, 256, 56, 56]          [1, 256, 56, 56]          (70,400)                  False\n",
       "│    │    └─Bottleneck: 3-3                        [1, 256, 56, 56]          [1, 256, 56, 56]          (70,400)                  False\n",
       "│    └─Sequential: 2-6                             [1, 256, 56, 56]          [1, 512, 28, 28]          --                        False\n",
       "│    │    └─Bottleneck: 3-4                        [1, 256, 56, 56]          [1, 512, 28, 28]          (379,392)                 False\n",
       "│    │    └─Bottleneck: 3-5                        [1, 512, 28, 28]          [1, 512, 28, 28]          (280,064)                 False\n",
       "│    │    └─Bottleneck: 3-6                        [1, 512, 28, 28]          [1, 512, 28, 28]          (280,064)                 False\n",
       "│    │    └─Bottleneck: 3-7                        [1, 512, 28, 28]          [1, 512, 28, 28]          (280,064)                 False\n",
       "│    └─Sequential: 2-7                             [1, 512, 28, 28]          [1, 1024, 28, 28]         --                        False\n",
       "│    │    └─Bottleneck: 3-8                        [1, 512, 28, 28]          [1, 1024, 28, 28]         (1,512,448)               False\n",
       "│    │    └─Bottleneck: 3-9                        [1, 1024, 28, 28]         [1, 1024, 28, 28]         (1,117,184)               False\n",
       "│    │    └─Bottleneck: 3-10                       [1, 1024, 28, 28]         [1, 1024, 28, 28]         (1,117,184)               False\n",
       "│    │    └─Bottleneck: 3-11                       [1, 1024, 28, 28]         [1, 1024, 28, 28]         (1,117,184)               False\n",
       "│    │    └─Bottleneck: 3-12                       [1, 1024, 28, 28]         [1, 1024, 28, 28]         (1,117,184)               False\n",
       "│    │    └─Bottleneck: 3-13                       [1, 1024, 28, 28]         [1, 1024, 28, 28]         (1,117,184)               False\n",
       "│    └─Sequential: 2-8                             [1, 1024, 28, 28]         [1, 2048, 28, 28]         --                        False\n",
       "│    │    └─Bottleneck: 3-14                       [1, 1024, 28, 28]         [1, 2048, 28, 28]         (6,039,552)               False\n",
       "│    │    └─Bottleneck: 3-15                       [1, 2048, 28, 28]         [1, 2048, 28, 28]         (4,462,592)               False\n",
       "│    │    └─Bottleneck: 3-16                       [1, 2048, 28, 28]         [1, 2048, 28, 28]         (4,462,592)               False\n",
       "├─DeepLabHead: 1-2                                 [1, 2048, 28, 28]         [1, 2, 28, 28]            --                        True\n",
       "│    └─ASPP: 2-9                                   [1, 2048, 28, 28]         [1, 256, 28, 28]          --                        True\n",
       "│    │    └─ModuleList: 3-17                       --                        --                        15,206,912                True\n",
       "│    │    └─Sequential: 3-18                       [1, 1280, 28, 28]         [1, 256, 28, 28]          328,192                   True\n",
       "│    └─Conv2d: 2-10                                [1, 256, 28, 28]          [1, 256, 28, 28]          589,824                   True\n",
       "│    └─BatchNorm2d: 2-11                           [1, 256, 28, 28]          [1, 256, 28, 28]          512                       True\n",
       "│    └─ReLU: 2-12                                  [1, 256, 28, 28]          [1, 256, 28, 28]          --                        --\n",
       "│    └─Conv2d: 2-13                                [1, 256, 28, 28]          [1, 2, 28, 28]            514                       True\n",
       "├─FCNHead: 1-3                                     [1, 1024, 28, 28]         [1, 2, 28, 28]            --                        True\n",
       "│    └─Conv2d: 2-14                                [1, 1024, 28, 28]         [1, 256, 28, 28]          2,359,296                 True\n",
       "│    └─BatchNorm2d: 2-15                           [1, 256, 28, 28]          [1, 256, 28, 28]          512                       True\n",
       "│    └─ReLU: 2-16                                  [1, 256, 28, 28]          [1, 256, 28, 28]          --                        --\n",
       "│    └─Dropout: 2-17                               [1, 256, 28, 28]          [1, 256, 28, 28]          --                        --\n",
       "│    └─Conv2d: 2-18                                [1, 256, 28, 28]          [1, 2, 28, 28]            514                       True\n",
       "======================================================================================================================================================\n",
       "Total params: 41,988,036\n",
       "Trainable params: 18,489,412\n",
       "Non-trainable params: 23,498,624\n",
       "Total mult-adds (G): 33.08\n",
       "======================================================================================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 425.52\n",
       "Params size (MB): 167.95\n",
       "Estimated Total Size (MB): 593.67\n",
       "======================================================================================================================================================"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Freeze backbone\n",
    "for param in deeplap_V3.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "    \n",
    "# Change classifier and auxiliary head to 2 only\n",
    "deeplap_V3.classifier[4] =  torch.nn.Conv2d(256, 2, # this is the only change\n",
    "                                            kernel_size=(1, 1), stride=(1, 1))\n",
    "deeplap_V3.aux_classifier[4] =  torch.nn.Conv2d(256, 2, # this is the only change\n",
    "                                                kernel_size=(1, 1), stride=(1, 1))\n",
    "\n",
    "# Change the input to be 1 channel only (not 3)\n",
    "deeplap_V3.backbone['conv1'] = torch.nn.Conv2d (1 # this is the only change\n",
    "                                                , 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "summary(\n",
    "    deeplap_V3,\n",
    "    # The input size should match the model's 2D configuration: (batch_size, channels, height, width)\n",
    "    input_size=(1, 1, 224, 224),\n",
    "    col_names=(\"input_size\", \"output_size\", \"num_params\", \"trainable\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c70f6c",
   "metadata": {},
   "source": [
    "## 2. Create dataset and dataloaders\n",
    "\n",
    "This section prepares the dataset and DataLoaders. Steps:\n",
    "\n",
    "1. Implement a custom Dataset that reads grayscale images and masks, binarizes masks, and applies optional transforms (e.g., resizing/augmentation).\n",
    "2. Instantiate the full dataset and split it into training and validation subsets (80/20 split).\n",
    "3. Create DataLoaders with appropriate batch sizes and worker counts for training and validation.\n",
    "\n",
    "Keep the transforms consistent between training and validation (except for any augmentations you only want during training).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f189d1",
   "metadata": {},
   "source": [
    "### 2.1 Dataset details\n",
    "\n",
    "The custom `BrainTumorDataset` does the following:\n",
    "\n",
    "- Loads images and masks from the provided directories and sorts filenames to keep pairs aligned.\n",
    "- Converts both image and mask to grayscale (`L` mode) and returns them as NumPy arrays.\n",
    "- Binarizes masks by thresholding at 127 (so 0/255 masks become 0/1).\n",
    "- If a transform is provided (e.g., Albumentations), it is applied to both image and mask together.\n",
    "\n",
    "Important: Ensure the dataset directories contain matching image/mask pairs and are named consistently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "807ecafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class BrainTumorDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset for loading brain tumor images and masks.\"\"\"\n",
    "    \n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (pathlib.Path or str): Directory with all the images.\n",
    "            mask_dir (pathlib.Path or str): Directory with all the masks.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Get all image and mask file paths and sort them to ensure they correspond\n",
    "        self.images = sorted(list(image_dir.glob(\"*.png\")))\n",
    "        self.masks = sorted(list(mask_dir.glob(\"*.png\")))\n",
    "        \n",
    "        # Ensure the number of images and masks are the same for data integrity\n",
    "        assert len(self.images) == len(self.masks), \"Number of images and masks must be equal.\"\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetches the sample at the given index, loads it, and applies transforms.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to fetch.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, mask) where image is the transformed image and mask is the transformed mask.\n",
    "        \"\"\"\n",
    "        # Get the file paths for the image and mask\n",
    "        image_path = self.images[idx]\n",
    "        mask_path = self.masks[idx]\n",
    "        \n",
    "        # Open the image and mask files\n",
    "        # Images and masks are converted to grayscale (L) for single-channel processing\n",
    "        image = np.array(Image.open(image_path).convert(\"L\"))\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n",
    "        \n",
    "        # Normalize mask to be 0 or 1, as pixel values are often 0 and 255\n",
    "        # A more robust and explicit way to binarize the mask\n",
    "        mask = (mask > 127).astype(np.float32)\n",
    "        \n",
    "        \"\"\"\n",
    "        (mask > 127): This creates a boolean array \n",
    "                    where pixels with a value greater than 127 become True \n",
    "                    and all others become False.\n",
    "                    \n",
    "        .astype(np.float32): This converts the boolean array into a float array, \n",
    "                            where True becomes 1.0 and False becomes 0.0.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        # Apply transformations if they are provided\n",
    "        if self.transform:\n",
    "            # Most transform libraries like Albumentations expect a dictionary\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed[\"image\"]\n",
    "            mask = transformed[\"mask\"]\n",
    "            \n",
    "        return image, mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a3757e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: <torch.utils.data.dataset.Subset object at 0x000002B245DB6F50>\n",
      "Validation dataset: <torch.utils.data.dataset.Subset object at 0x000002B245DB4A30>\n",
      "Total samples: 3064\n",
      "Training samples: 2451\n",
      "Validation samples: 613\n"
     ]
    }
   ],
   "source": [
    "# init datasets\n",
    "from pathlib import Path\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "IMAGES_DIR = Path(\"brain_tumor_dataset/images\")\n",
    "MASKS_DIR = Path(\"brain_tumor_dataset/masks\")\n",
    "\n",
    "# 1. Instantiate the full dataset\n",
    "full_dataset = BrainTumorDataset(image_dir= IMAGES_DIR,\n",
    "                            mask_dir= MASKS_DIR,\n",
    "                            transform= deeplab_V3_transform)\n",
    "\n",
    "# 2. Define split sizes (80% train, 20% validation)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "# 3. Perform the split\n",
    "generator = torch.Generator().manual_seed(42) # for reproducibility\n",
    "train_dataset, val_dataset = random_split(full_dataset, \n",
    "                                        [train_size, val_size], \n",
    "                                        generator=generator)\n",
    "\n",
    "# Test print lengths\n",
    "print(f\"Train dataset: {train_dataset}\")\n",
    "print(f\"Validation dataset: {val_dataset}\")\n",
    "\n",
    "print(f\"Total samples: {len(full_dataset)}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cd12a9",
   "metadata": {},
   "source": [
    "### 2.2 DataLoaders\n",
    "\n",
    "We create PyTorch DataLoaders for training and validation. Important considerations:\n",
    "\n",
    "- Use `shuffle=True` for the training loader to randomize samples each epoch.\n",
    "- Use reasonable `batch_size` values based on GPU memory.\n",
    "- Set `num_workers` to a value appropriate for your machine (e.g., `os.cpu_count()`), and use `pin_memory=True` when training on CUDA.\n",
    "\n",
    "The loaders return (image, mask) tuples ready for the training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffde3b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x2b245db5180>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x2b245db62c0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "train_loader = DataLoader(dataset= train_dataset,\n",
    "                        batch_size= 8,\n",
    "                        shuffle= True,\n",
    "                        pin_memory= True,\n",
    "                        num_workers= os.cpu_count())\n",
    "\n",
    "val_loader = DataLoader(dataset= val_dataset,\n",
    "                        batch_size= 4,\n",
    "                        shuffle= False,\n",
    "                        pin_memory= True,\n",
    "                        num_workers= os.cpu_count()//2)\n",
    "\n",
    "train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1361035",
   "metadata": {},
   "source": [
    "## 3. Loss function, optimizer, and metrics\n",
    "\n",
    "Choices and rationale:\n",
    "\n",
    "- Loss: MONAI's `DiceLoss(sigmoid=True)` is used because Dice is appropriate for segmentation and encourages overlap between predictions and ground truth. `sigmoid=True` applies a per-pixel sigmoid to raw logits.\n",
    "- Optimizer: Adam with a small learning rate (1e-4) is a good default for fine-tuning segmentation heads.\n",
    "- Scheduler: `ReduceLROnPlateau` reduces the learning rate if validation loss plateaus, helping escape local minima.\n",
    "- Metric: MONAI's `DiceMetric` (with `include_background=False`) evaluates segmentation quality excluding background to focus on tumor detection.\n",
    "- AMP: Automatic Mixed Precision via `torch.amp.GradScaler` speeds up training on modern GPUs and reduces memory usage.\n",
    "\n",
    "We also keep a `best_val_dice` to checkpoint the best-performing model during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a67a1893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "\n",
    "\n",
    "# Define the loss function. DiceLoss is excellent for segmentation tasks.\n",
    "# sigmoid=True is necessary as our model outputs raw logits. \n",
    "# It applies a sigmoid to the output before calculating loss.\n",
    "loss_fn = DiceLoss(sigmoid=True)\n",
    "\n",
    "# Define the optimizer.\n",
    "optimizer = torch.optim.Adam(deeplap_V3.parameters(), \n",
    "                            lr=1e-4)\n",
    "\n",
    "# Define a learning rate scheduler. \n",
    "# It will reduce the learning rate when the validation loss stops improving ('min' mode).\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                        mode='min', \n",
    "                                                        factor=0.1, \n",
    "                                                        patience=10)\n",
    "\n",
    "\n",
    "# Initialize the Dice Metric for evaluating segmentation performance.\n",
    "# `include_background=False` means the background class (label 0) is ignored during calculation.\n",
    "# `reduction=\"mean\"` specifies that the Dice scores for all classes will be averaged.\n",
    "metric = DiceMetric(include_background=False,\n",
    "                    reduction= \"mean\")\n",
    "\n",
    "# Set the total number of training epochs.\n",
    "epochs = 20\n",
    "\n",
    "# Initialize a variable to track the best validation Dice score achieved so far.\n",
    "# This is used to save the best performing model checkpoint.\n",
    "best_val_dice = 0.0\n",
    "\n",
    "# Initialize a Gradient Scaler for Automatic Mixed Precision (AMP) training.\n",
    "# This helps prevent underflow of gradients when using float16 precision,\n",
    "# improving training stability and speed on compatible GPUs.\n",
    "scaler = torch.amp.GradScaler(device = \"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182c9859",
   "metadata": {},
   "source": [
    "## 4. Training loop\n",
    "\n",
    "Training uses a modular engine (imported from `going_modular.engine`) to encapsulate the training and validation steps. Key points:\n",
    "\n",
    "- The engine handles forward/backward passes, loss computation, metric updates, AMP scaling, scheduler steps, and checkpointing.\n",
    "- Provide model, data loaders, loss function, optimizer, scheduler, scaler, device, number of epochs, and a checkpoint directory.\n",
    "- Checkpointing saves the best model (by validation Dice) so you can resume or evaluate the best weights later.\n",
    "\n",
    "After training, inspect the saved checkpoints and validation metrics to assess performance. Consider unfreezing parts of the backbone and fine-tuning with a lower learning rate if more improvement is needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3412c0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Starting Epoch 1/20 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E-1:   0%|          | 0/307 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from going_modular import engine\n",
    "\n",
    "results = engine.train(model = deeplap_V3,\n",
    "                    train_loader = train_loader,\n",
    "                    test_loader = val_loader, \n",
    "                    loss_fn = loss_fn, \n",
    "                    optimizer = optimizer, \n",
    "                    scheduler = scheduler,\n",
    "                    scaler = scaler, \n",
    "                    device = \"cuda\" if torch.cuda.is_available() else \"cpu\", \n",
    "                    epochs = epochs,\n",
    "                    checkpoint_dir = \"checkpoints_2/ \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865f798e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
